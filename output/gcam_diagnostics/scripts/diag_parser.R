# LEGAL NOTICE
# This computer software was prepared by Battelle Memorial Institute,
# hereinafter the Contractor, under Contract No. DE-AC05-76RL0 1830
# with the Department of Energy (DOE). NEITHER THE GOVERNMENT NOR THE
# CONTRACTOR MAKES ANY WARRANTY, EXPRESS OR IMPLIED, OR ASSUMES ANY
# LIABILITY FOR THE USE OF THIS SOFTWARE. This notice including this
# sentence must appear on any copies of this computer software.
# 
# EXPORT CONTROL
# User agrees that the Software will not be shipped, transferred or
# exported into any country or used in any manner prohibited by the
# United States Export Administration Act or any other applicable
# export laws, restrictions or regulations (collectively the "Export Laws").
# Export of the Software may require some form of license or other
# authority from the U.S. Government, and failure to obtain such
# export control license may result in criminal liability under
# U.S. laws. In addition, if the Software is identified as export controlled
# items under the Export Laws, User represents and warrants that User
# is not a citizen, or otherwise located within, an embargoed nation
# (including without limitation Iran, Syria, Sudan, Cuba, and North Korea)
#     and that User is not otherwise prohibited
# under the Export Laws from receiving the Software.
# 
# Copyright 2011 Battelle Memorial Institute.  All Rights Reserved.
# Distributed as open-source under the terms of the Educational Community 
# License version 2.0 (ECL 2.0). http://www.opensource.org/licenses/ecl2.php
# 
# For further details, see: http://www.globalchange.umd.edu/models/gcam/
#

# diag_parser.R
#
# An automated graphing system to process GCAM output data (as generated by the
# ModelInterface) and generate both standard and user-defined graphs.
#
# Ben Bond-Lamberty, November 2012

# -----------------------------------------------------------------------------
# parse_mi_output: parse the ModelInterface output
# It tries to be smart, only reading the tables that will actually be needed
parse_mi_output <- function( fn ) {

    tables <- list()

    # See if the user has provided any values overriding our defaults   
    use_tablenames <- TRUE
    headerline <- "scenario"
    yearpat <- "X[0-9]{4}"
        
    printlog( "Reading", fn, "...", cr=F, level=LOGLEVEL_SUMMARY )
    tryCatch( {
        fdata <- scan( fn, what=character(), sep="\n", blank.lines.skip=F, quiet=T )    
    }, error=function( err ) {
        printlog( "error reading file" )
        printlog( as.character( err ) )
        stop()
    } )
    
    printlog( "OK.", ts=F )
    tableheaders <- grep( headerline, fdata )
    printlog( "Table headers located in lines", tableheaders )
    table_name <- NA
    
    for( i in 1:length( tableheaders ) ) {
        if( use_tablenames ) {
            table_name <- fdata[ tableheaders[ i ]-1 ]
            
            if( table_name %in% names( tables ) ) {
                printlog( "Table name", table_name, "has already been read!" )
                stop()
            }
        } else {
            table_name <- i
        }
        printlog( "Table", i, "name is", table_name )
#       if( !( table_name %in% tables_needed ) ) {
#           printlog( "Table", table_name, "not needed, skipping" )
#           next
#       }

        nskip <- tableheaders[ i ] - 1
        headers <- fdata[ tableheaders[ i ] ]
        extrafields <- 0
        while( substr( headers, nchar( headers), nchar( headers ) )=="," ) {
            headers <- substr( headers, 1, nchar( headers )-1 )
            extrafields <- extrafields + 1
        }
#       printlog( "Extra fields =", extrafields )

        if( i==length( tableheaders ) )
            nrows <- -1
        else
            nrows <- tableheaders[ i+1 ] - tableheaders[ i ] - 1 - use_tablenames       # i.e., subtract 1 if using table names

        printlog( "Reading table", i, "in", fn, "( skip =", nskip, " nrows =", nrows, ")" )
        tempdata <- read.table( fn, row.names=NULL, skip=nskip, nrows=nrows, header=T, 
            stringsAsFactors=F, sep=",", comment.char=GCAM_DATA_COMMENT ) #stringsAsFactors=F, 
        
        # Remove extra columns on end - this is often present in the MI output
        if( extrafields > 0 ) {
            printlog( "Removing", extrafields, "extra fields" )
            tempdata <- tempdata[ -seq( ncol( tempdata ) - extrafields+1, ncol( tempdata ) ) ]
        }
        
        printlog( "nrow =", nrow( tempdata ), "ncol =", ncol( tempdata ) )

        # Extract scenario and date
        if( SCENARIO_FIELD_NAME %in% names( tempdata ) ) {
#           printlog( "Separating scenario and date" )
            splitinfo <- str_split_fixed( tempdata[ , SCENARIO_FIELD_NAME ], ",date=", 2 )
            tempdata$scenario <- splitinfo[ , 1 ]
            tempdata$date <- splitinfo[ , 2 ]
        }
        
        # Extract AEZ, if available - this is very specific and breakable
        searchstring <- "AEZ[0-9]{1}"
        splitstring <- "AEZ"
        if( !is.na( splitstring ) ) {
#           printlog( "Looking for", splitstring )
            for( j in 1:ncol( tempdata ) ) {
                if( grepl( searchstring, tempdata[ 1, j ] ) ) {
                    printlog( splitstring, "data located in column", j )
                    dtemp <- as.data.frame( str_split_fixed( tempdata[ , j ], splitstring, 2 ) )
#                   print(head(dtemp))
                    tempdata[ , j ] <- dtemp[ , 1 ]
                    tempdata[ splitstring ] <- dtemp[,2] #as.factor( as.numeric( dtemp[ , 2 ] ) )
                }
            }
        }

#       printlog( "Melting..." )
        yearfields <- grep( yearpat, names( tempdata ) )
        tempdata <- melt( tempdata, measure.vars=yearfields )

        # Make a better year field and assign file name field
        tempdata$Year <- as.numeric( substr( tempdata$variable, 2, length( tempdata$variable ) ) )
        tempdata$variable <- NULL
        tempdata[ , FILE_FIELD_NAME ] <- strsplit( basename( fn ), ".csv" )[[ 1 ]][ 1 ]

        tables[[ table_name ]] <- tempdata
    } # for i

    return( tables )
} # parse_mi_output

